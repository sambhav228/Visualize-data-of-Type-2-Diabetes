# Import Python Packages
import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import itertools
from sklearn import model_selection
from sklearn.impute import SimpleImputer
# Define Helper Functions
def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,n_jobs=1,
train_sizes=np.linspace(.1, 1.0, 5)):

 plt.figure()
 plt.title(title)
 if ylim is not None:
 plt.ylim(*ylim)
 plt.xlabel("Training examples")
 plt.ylabel("Score")
 train_sizes, train_scores, test_scores = learning_curve(
 estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)
 train_scores_mean = np.mean(train_scores, axis=1)
 train_scores_std = np.std(train_scores, axis=1)
 test_scores_mean = np.mean(test_scores, axis=1)
 test_scores_std = np.std(test_scores, axis=1)
 plt.grid()
 plt.fill_between(train_sizes, train_scores_mean - train_scores_std,
 train_scores_mean + train_scores_std, alpha=0.1,
 color="r")
 plt.fill_between(train_sizes, test_scores_mean - test_scores_std,
 test_scores_mean + test_scores_std, alpha=0.1, color="g")
 plt.plot(train_sizes, train_scores_mean, 'o-', color="r",
 label="Training score")
 plt.plot(train_sizes, test_scores_mean, 'o-', color="g",
 label="Cross-validation score")
 plt.legend(loc="best")
 return plt
 
 def plot_confusion_matrix(cm, classes,
 normalize=False,
 title='Confusion matrix',
 cmap=plt.cm.Blues):
 plt.imshow(cm, interpolation='nearest', cmap=cmap)
 plt.title(title)
 plt.colorbar()
 tick_marks = np.arange(len(classes))
 plt.xticks(tick_marks, classes, rotation=45)
 plt.yticks(tick_marks, classes)
 fmt = '.2f' if normalize else 'd'
 thresh = cm.max() / 2.
 for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
 plt.text(j, i, format(cm[i, j], fmt),
 horizontalalignment="center",
 color="white" if cm[i, j] > thresh else "black")
 plt.tight_layout()
 plt.ylabel('True label')
 plt.xlabel('Predicted label') 
 
 def compareABunchOfDifferentModelsAccuracy(a, b, c, d):

 print('\nCompare Multiple Classifiers: \n')
 print('K-Fold Cross-Validation Accuracy: \n')
 names = []
 models = []
 resultsAccuracy = []
 models.append(('LR', LogisticRegression()))
 models.append(('RF', RandomForestClassifier()))
 models.append(('KNN', KNeighborsClassifier()))
 models.append(('SVM', SVC()))
 models.append(('LSVM', LinearSVC()))
 models.append(('GNB', GaussianNB()))
 models.append(('DTC', DecisionTreeClassifier()))
 models.append(('GBC', GradientBoostingClassifier()))
 for name, model in models:
 model.fit(a, b)
 kfold = model_selection.KFold(n_splits=10, random_state=7)
 accuracy_results = model_selection.cross_val_score(model, a,b, cv=kfold,scoring='accuracy')
 resultsAccuracy.append(accuracy_results)
 names.append(name)
 accuracyMessage = "%s: %f (%f)" % (name, accuracy_results.mean(),
accuracy_results.std())
 print(accuracyMessage)
 # Boxplot
 fig = plt.figure()
 fig.suptitle('Algorithm Comparison: Accuracy')
 ax = fig.add_subplot(111)
 plt.boxplot(resultsAccuracy)
 ax.set_xticklabels(names)
 ax.set_ylabel('Cross-Validation: Accuracy Score')
 plt.show()

def defineModels():
 print('\nLR = LogisticRegression')
 print('RF = RandomForestClassifier')
 print('KNN = KNeighborsClassifier')
 print('SVM = Support Vector Machine SVC')
 print('LSVM = LinearSVC')
 print('GNB = GaussianNB')
 print('DTC = DecisionTreeClassifier')
 print('GBC = GradientBoostingClassifier \n\n')
names = ["Nearest Neighbors", "Linear SVM", "RBF SVM", "Gaussian Process",
 "Decision Tree", "Random Forest", "MLPClassifier", "AdaBoost",
 "Naive Bayes", "QDA"]
classifiers = [
 KNeighborsClassifier(),
 SVC(kernel="linear"),
 SVC(kernel="rbf"),
 GaussianProcessClassifier(),
 DecisionTreeClassifier(),
 RandomForestClassifier(),
 MLPClassifier(),
 AdaBoostClassifier(),
 GaussianNB(),
 QuadraticDiscriminantAnalysis()
]

dict_characters = {0: 'Healthy', 1: 'Diabetes'}
dataset = read_csv('diabetes.csv')
dataset.head(10)
def plotHistogram(values,label,feature,title):
 sns.set_style("whitegrid")
 plotOne = sns.FacetGrid(values, hue=label,aspect=2)
 plotOne.map(sns.distplot,feature,kde=False)
 plotOne.set(xlim=(0, values[feature].max()))
 plotOne.add_legend()
 plotOne.set_axis_labels(feature, 'Proportion')
 plotOne.fig.suptitle(title)
 plt.show()
plotHistogram(dataset,"Outcome",'Insulin','Insulin vs Diagnosis (Blue = Healthy;
Orange = Diabetes)')
plotHistogram(dataset,"Outcome",'SkinThickness','SkinThickness vs Diagnosis (Blue
= Healthy; Orange = Diabetes)')
dataset2 = dataset.iloc[:, :-1]
print("# of Rows, # of Columns: ",dataset2.shape)
print("\nColumn Name # of Null Values\n")
print((dataset2[:] == 0).sum())
print("# of Rows, # of Columns: ",dataset2.shape)
print("\nColumn Name % Null Values\n")
print(((dataset2[:] == 0).sum())/768*100)
g = sns.heatmap(dataset.corr(),cmap="BrBG",annot=False)
dataset.corr()
 
